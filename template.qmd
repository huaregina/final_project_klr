---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

## Background

## Data

\newpage

```{python}
#| echo: false
import os
import pandas as pd
import altair as alt
import unittest
import time
import datetime as dt
import geopandas as gpd
from shapely import wkt
from shapely.geometry import Point
import numpy as np
import json

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
alt.data_transformers.disable_max_rows() 
```


## Data Cleaning and Benchmarking

1. We collected data on City-owned land from the City of Chicago and property data from the Cook County Assessorâ€™s Office, followed by initial data cleaning to prepare the datasets for analysis.

```{python}
# | eval: false

# load the property data in cook county
data_cook_county = pd.read_csv('/Users/aa/Documents/Q4/data_cook_county.csv')

# filter only data in Chicago
# Chicago zipcode start with '606' except Riverdale(60827) and Galewood(60707)
data_cook_county['zip_code'] = data_cook_county['zip_code'].astype(str)

data_chi = data_cook_county[
    (data_cook_county['zip_code'].str.startswith('606')) |
    (data_cook_county['zip_code'] == '60827') |
    (data_cook_county['zip_code'] == '60707')
]

# filter only data of vacant lands (property class code started with 1)
data_chi_vacant = data_chi[data_chi['class'].str.startswith('1')]

# remove unneccessary columns
selected_columns = [
    'pin', 'pin10', 'tax_year', 'class', 'township_name', 'township_code',
    'neighborhood_code', 'tax_district_code', 'zip_code', 'longitude', 'latitude',
    'chicago_community_area_num', 'chicago_community_area_name', 'enterprise_zone_num'
]

data_chi_vacant = data_chi_vacant[selected_columns]

# save it to the repo
data_chi_vacant.to_csv('/Users/aa/Documents/GitHub/final_project_klr/data/data_ccao.csv', index=False)
```

```{python}
# read in the data
df_ccao = pd.read_csv('/Users/aa/Documents/GitHub/final_project_klr/data/data_ccao.csv')
df_coc = pd.read_csv('/Users/aa/Documents/GitHub/final_project_klr/data/df_city_owned.csv')
```

```{python}
df_coc = df_coc[df_coc['Zip Code'] != 0]
df_coc = df_coc[df_coc['Property Status'] == 'Owned by City']
df_coc = df_coc.dropna(subset=['Zip Code'])
```


2. How many total sqft of vacant land in Chicago and how much of the value?

```{python}
print(f"The total area of vacant land is {sum(df_coc['Sq. Ft.'])}")
```

```{python}
# Compute counts and average value for each community area
top_community_areas = (
    df_coc.groupby('Community Area Name')
    .agg(counts=('Community Area Name', 'size'), avg_value=('Land Value (2022)', 'mean'))
    .reset_index()
    .sort_values('counts', ascending=False)
    .head(15)
)

# Create a bar chart for counts
top_community_bar = alt.Chart(top_community_areas).mark_bar(color='rgb(151, 188, 193)').encode(
    x=alt.X('counts:Q', title='Count', axis=alt.Axis(titleFontSize=12)),
    y=alt.Y('Community Area Name:N', sort='-x', title='Community Area Name'),
    tooltip=['Community Area Name', 'counts']
)

# Create a line chart for average value
top_community_line = alt.Chart(top_community_areas).mark_line(color='rgb(222, 118, 113)').encode(
    x=alt.X('avg_value:Q', title = None, axis = None),
    y=alt.Y('Community Area Name:N', sort='-x', title=None),  # Hide line chart y-axis
    tooltip=['Community Area Name', 'avg_value']
)

vertical_line_20k = alt.Chart(top_community_areas).mark_rule(color='grey', strokeDash=[5, 5]).encode(
    x=alt.value(55)
)

vertical_line_100k = alt.Chart(top_community_areas).mark_rule(color='grey', strokeDash=[5, 5]).encode(
    x=alt.value(273)
)

# Add labels for the vertical lines
label_20k = alt.Chart(pd.DataFrame({'x': [20000], 'y': [0], 'label': ['20k']})).mark_text(
    text='$20,000', align='left', dx=5, dy=-15, color='grey'
).encode(
    x=alt.value(55),
    y=alt.value(30)  # Adjust label position vertically
)

label_100k = alt.Chart(pd.DataFrame({'x': [100000], 'y': [0], 'label': ['100k']})).mark_text(
    text='$100,000', align='left', dx=5, dy=-15, color='grey'
).encode(
    x=alt.value(215),
    y=alt.value(250)  # Adjust label position vertically
)

# Combine the bar and line charts
alt.layer(top_community_bar, 
          top_community_line,
          vertical_line_20k, 
          vertical_line_100k,
          label_20k,
          label_100k
          ).resolve_scale(
    y='shared',  # Share the y-axis between the bar and line charts
    x='independent'  # Use independent x-axes
).properties(
    title='Top 15 Community Areas by Counts and Average Value',
    width=300,
    height=300
)

```


3. Is all data in City of Chicago a subset of the Cook County Data?

We find almost all the pin codes from the City of Chicago data is not in the Cook County data so we will combine the two dataset.

```{python}
# The 'pin' column in df_coc is not in the same format as that in ccao
df_coc['PIN'] = df_coc['PIN'].str.replace('-','')
df_ccao['pin'] = df_ccao['pin'].astype(str)

# Find the set of PINs in df_coc but not in df_ccao
missing_pins = set(df_coc['PIN']) - set(df_ccao['pin'])

# Count the number of missing PINs
missing_count = len(missing_pins)

print(f"Number of PINs in df_coc['PIN'] not in df_ccao['pin']: {missing_count}")
```

```{python}
selected_columns_coc = [
    'PIN','Community Area Number', 'Community Area Name', 'Zip Code', 'Latitude', 'Longitude'
]

df_coc = df_coc[selected_columns_coc]

selected_columns_ccao = [
    'pin','chicago_community_area_num', 'chicago_community_area_name', 'zip_code', 'latitude', 'longitude'
]

df_ccao = df_ccao[selected_columns_ccao]
```

```{python}
# rename some of the columns
df_coc = df_coc.rename(columns={
    'PIN': 'pin',
    'Community Area Number': 'chicago_community_area_num',
    'Community Area Name': 'chicago_community_area_name',
    'Zip Code': 'zip_code',
    'Latitude': 'latitude',
    'Longitude': 'longitude'
})
```

```{python}
df_coc = df_coc.dropna()

df_coc['type'] = 'City-Owned'
df_ccao['type'] = 'Private'
```

```{python}
df = pd.concat([df_ccao, df_coc], axis=0).drop_duplicates(subset='pin')
```

```{python}
df.head()
```


# 3. Display vacant lots by scatter plot

## Choice1: By Binned coordinates

```{python}
# Define the binning function with a step size of 0.01
def bin_coordinates(coord, step=0.01):
    return np.floor(coord / step) * step

# Apply binning to Latitude and Longitude columns
df['Binned_Latitude'] = bin_coordinates(df['latitude'], step=0.01)
df['Binned_Longitude'] = bin_coordinates(df['longitude'], step=0.01)

# Count occurrences of each binned coordinate combination
coordinate_counts = df.groupby(['Binned_Latitude', 'Binned_Longitude']).size().reset_index(name='Count')
```

```{python}
# Ensure scatter data is in the correct format
scatter_data = coordinate_counts.copy()
scatter_data = scatter_data.rename(columns={'Binned_Latitude': 'Latitude', 'Binned_Longitude': 'Longitude'})

# Create scatter plot with consistent projection
scatter_bin = alt.Chart(scatter_data).mark_circle(color='blue', opacity=0.5).encode(
    longitude='Longitude:Q',
    latitude='Latitude:Q',
    size=alt.Size('Count:Q', title='Vacant Lands Counts', scale=alt.Scale(range=[10, 300])),
    tooltip=['Latitude:Q', 'Longitude:Q', 'Count:Q']
).project(
    type='identity',
    reflectY=True
).properties(
    width=400,
    height=400
)

scatter_bin
```

## Choice2: By raw data points

```{python}
scatter = alt.Chart(df).mark_point(size=0.3, filled=True, color='blue').encode(
    longitude='longitude',
    latitude='latitude'
).project(
    type='identity',
    reflectY=True
).properties(
    title='Vacant Land Locations'
)

scatter
```

# 4. How is vacant lots related with other characteristics of the community?

## 4.1 CTA lines (Zip Code)
```{python}
# Read the csv file
df_city = pd.read_csv('City-Owned_Land_Inventory_20241130.csv')  # I just use the city-owned data here
```

# Some data cleaning progress
```{python}
# Filter out properties without zip codes
df0 = df_city[~df_city['Zip Code'].isna()]

df0.columns = df0.columns.str.strip()

# A quick check
df0 = df0.copy()
df0['Latitude'] = pd.to_numeric(df0['Latitude'], errors='coerce')
df0['Longitude'] = pd.to_numeric(df0['Longitude'], errors='coerce')
# print(df0.dtypes)

# Transform data type
df0['Zip Code'] = df0['Zip Code'].astype(int)

# Check the dimensions
print(df0.shape)
print(df0.dtypes)
```
```{python}
# Filter by property status
df1 = df0[df0['Property Status'] == 'Owned by City']

# Check the dimensions
print(df1.shape)
```

## Load data
```{python}
# Read shapefile using GeoPandas
shapefile_path2 = r"C:\Users\Yuzi\Documents\GitHub\VacantLots\ZIP Codes\geo_export.shp"
gdf2 = gpd.read_file(shapefile_path2)

# Transform data type
gdf2['zip'] = pd.to_numeric(gdf2['zip'], errors='coerce').fillna(0).astype(int)

# A quick check
print(gdf2.head())
print(gdf2.dtypes)
```

```{python}
# Copy the dataset
df_vacant = df2.copy()

# Ensure 'Zip Code' column is cleaned and valid
df_vacant['Cleaned_Zip'] = df_vacant['Zip Code'].astype(str).str.zfill(5)  # Ensure Zip Codes are strings of 5 digits

# Drop rows with missing or invalid Zip Codes
df_vacant_clean = df_vacant.dropna(subset=['Cleaned_Zip'])

# Count the number of vacant lots per Zip Code
zip_counts = df_vacant_clean['Cleaned_Zip'].value_counts().reset_index()
zip_counts.columns = ['Zip', 'Vacant_Lots_Count']  # Rename columns for clarity

# Check the first few rows of the counts DataFrame
# print(zip_counts.head())

```

```{python}
# Read shapefile using GeoPandas
shapefile_path2 = r"C:\Users\Yuzi\Documents\GitHub\VacantLots\ZIP Codes\geo_export.shp"
geo_data = gpd.read_file(shapefile_path2)

# A quick check
print(geo_data.head())
```
```{python}
# Ensure 'zip' column in geo_data is cleaned for matching
geo_data['zip'] = geo_data['zip'].astype(str).str.zfill(5)  # Ensure Zip Codes are strings of 5 digits

# Merge geographic data with the counts
geo_data_counts = geo_data.merge(zip_counts, left_on='zip', right_on='Zip', how='left')

# Fill missing values for counts with 0
geo_data_counts['Vacant_Lots_Count'] = geo_data_counts['Vacant_Lots_Count'].fillna(0)
```

```{python}
# Convert GeoDataFrame to JSON-compatible format
geojson_data = geo_data_counts.to_crs(epsg=4326).to_json()  # Convert CRS to WGS84 for Altair compatibility

# Create the base map layer
base_map = alt.Chart(alt.Data(values=json.loads(geojson_data)['features'])).mark_geoshape(
    fill='white',
    stroke='gray',
    strokeWidth=0.5
).project(
    type='albersUsa'  # Use Albers USA projection for visualization
).properties(
    width=600,
    height=400
)

# Create the choropleth layer for vacant lot counts
choropleth = alt.Chart(alt.Data(values=json.loads(geojson_data)['features'])).mark_geoshape(
    stroke='white',
    strokeWidth=0.5
).encode(
    color=alt.Color('properties.Vacant_Lots_Count:Q', scale=alt.Scale(scheme='blues'), title='Vacant Lots Count'),
    tooltip=[
        alt.Tooltip('properties.zip:O', title='Zip Code'),
        alt.Tooltip('properties.Vacant_Lots_Count:Q', title='Vacant Lots Count')
    ]
).project(
    type='albersUsa'
)

# Combine base map and choropleth
final_map = (base_map + choropleth).properties(
    title={
        'text': 'Vacant Lots by Zip Code',
        'fontSize': 16,
        'anchor': 'middle'
    }
).configure_view(
    stroke=None  # Remove gridlines
)

# Show the final map
final_map

```

```{python}
# Read railLine data
railLine_path = r"C:\Users\Yuzi\Documents\GitHub\VacantLots\CTA _RailLines\geo_export.shp"
railLine_data = gpd.read_file(railLine_path)

# Convert GeoPandas data to GeoJSON for Altair
railLine_geojson = json.loads(railLine_data.to_crs(epsg=4326).to_json())

# Create Altair layer for rail lines
rail_lines_layer = alt.Chart(alt.Data(values=railLine_geojson['features'])).mark_geoshape(
    stroke='orange',
    strokeWidth=1
).project(
    type='albersUsa'  # Ensure projection matches the base map
)

# Combine final_map (base + choropleth) with rail lines
combined_map = (final_map + rail_lines_layer).properties(
    title={
        'text': 'Vacant Lots and Rail Lines by Zip Code',
        'fontSize': 16,
        'anchor': 'middle'
    }
)

# Show the combined map
combined_map

```
```{python}
# Load data and assign colors
railLine_path = r"C:\Users\Yuzi\Documents\GitHub\VacantLots\CTA _RailLines\geo_export.shp"
railLine_data = gpd.read_file(railLine_path)
color_mapping = {
    "ML": "gray",
    "BL": "blue",
    "BR": "brown",
    "GR": "green",
    "OR": "orange",
    "PK": "pink",
    "PR": "purple",
    "RD": "red",
    "YL": "yellow"
}
railLine_data['color'] = railLine_data['legend'].map(color_mapping)

# Transform railLine_data to GeoJSON
railLine_geojson = railLine_data.to_crs(epsg=4326).to_json()
railLine_features = json.loads(railLine_geojson)['features']

# Draw rail_layer
rail_layer = alt.Chart(alt.Data(values=railLine_features)).mark_geoshape(
    filled=False,
    strokeWidth=1.5
).encode(
    color=alt.Color('properties.legend:N', scale=alt.Scale(domain=list(color_mapping.keys()), range=list(color_mapping.values())),
                    legend=None),  # remove legend for 
    tooltip=[alt.Tooltip('properties.legend:N', title='Rail Line')]
).project(
    type='albersUsa'
)

# Draw Choropleth
geojson_data = geo_data_counts.to_crs(epsg=4326).to_json()
choropleth = alt.Chart(alt.Data(values=json.loads(geojson_data)['features'])).mark_geoshape(
    stroke='white',
    strokeWidth=0.5
).encode(
    color=alt.Color('properties.Vacant_Lots_Count:Q', scale=alt.Scale(scheme='blues'), title='Vacant Lots Count'),
    tooltip=[
        alt.Tooltip('properties.zip:O', title='Zip Code'),
        alt.Tooltip('properties.Vacant_Lots_Count:Q', title='Vacant Lots Count')
    ]
).project(
    type='albersUsa'
)

# Combine layers
final_map_combined = (choropleth + rail_layer).properties(
    title={
        'text': 'Vacant Lots by Zip Code with Rail Lines',
        'fontSize': 16,
        'anchor': 'middle'
    }
).configure_view(
    stroke=None
)

# Show the map
final_map_combined.show()

```


## 4.2 Crime Rate (Zip Code)

```{python}
# read Chicago zip code shapefile
gdf_zip = gpd.read_file('./data/Zip Codes/geo_export.shp')

# read crime data for 2023
df_crime = pd.read_csv('./data/Crimes_2023.csv')

# read pop data
df_pop = pd.read_csv('./data/Chicago_Population_Counts.csv')
```

```{python}
#### identify the zip code for each crime from its latitude&longitude

# drop rows with missing locations
df_crime = df_crime.dropna(subset=['Latitude', 'Longitude'])

# Create a 'geometry' column by applying Point to each latitude and longitude
df_crime['geometry'] = df_crime.apply(lambda row: Point(row['Longitude'], row['Latitude']), axis=1)

# convert df_crime to a geodataframe
gdf_crime = gpd.GeoDataFrame(df_crime, geometry='geometry', crs='EPSG:4326')

# Ensure zip code GeoDataFrame is also in the same crs
gdf_zip = gdf_zip.to_crs('EPSG:4326')

# Spatial join to assign zip codes to crimes
gdf_crime = gpd.sjoin(gdf_crime, gdf_zip, how='left', predicate='within')

gdf_crime.head()
```

```{python}
crime_count_zip = gdf_crime.groupby('zip').size().reset_index(name='crime_count')
```

```{python}
# Merge the crime count data with the population data (2021) using the correct column
df_pop_2021 = df_pop[df_pop['Year']==2021]
df_crime_rate = crime_count_zip.merge(df_pop_2021[['Geography','Population - Total']], left_on='zip', right_on='Geography')
```

```{python}
## there is no population data on zip code 60666 --> might have to try the full census data
df_pop[df_pop['Geography']==60666]
```

```{python}
# rename population column
df_crime_rate = df_crime_rate.rename(columns={'Population - Total': 'population'})

# Drop the 'Geography' column
df_crime_rate = df_crime_rate.drop(columns=['Geography'])

# Calculate the crime rate per 100 people
df_crime_rate['crime_rate'] = df_crime_rate['crime_count'] / df_crime_rate['population'] * 1000

# replace inf with na
df_crime_rate['crime_rate'] = df_crime_rate['crime_rate'].replace([float('inf'), -float('inf')], float('nan'))

df_crime_rate
```

```{python}
# merge with zip shapefile
gdf_crime_rate = gdf_zip.merge(df_crime_rate, on='zip', how='left')
# Fill missing crime rate with 0
gdf_crime_rate['crime_rate'] = gdf_crime_rate['crime_rate'].fillna(0).astype(int)

# Define the Windsorization limits 
lower_limit = 0.01  
upper_limit = 0.96  

# Apply Windsorization
gdf_crime_rate['crime_rate_windsorized'] = winsorize(
    gdf_crime_rate['crime_rate'], 
    limits=(lower_limit, 1 - upper_limit)
)

# Print before and after comparison
print(gdf_crime_rate[['crime_rate', 'crime_rate_windsorized']].describe())

```

```{python}
crime_rate_map = alt.Chart(gdf_crime_rate
).mark_geoshape(
    stroke='black',
    strokeWidth=0.5
).encode(
    color=alt.Color('crime_rate_windsorized:Q', 
                    scale=alt.Scale(range=["transparent", "red"]),  
                    title='Crime rate per 1,000 population'),
    tooltip=['zip:N', 'crime_rate_windsorized:Q']
).project(
    type='albersUsa'  
).properties(
    width=600,
    height=400,
)

crime_rate_map
```



```{python}
# Combine the choropleth map and scatter layers
combined_map_crime = (crime_rate_map + scatter).properties(
    title={
        'text': 'Crime rate by zip code and Vacant Lands',
        'fontSize': 16,
        'anchor': 'middle'
    }
).configure_view(
    stroke=None  # Remove gridlines
)

# Display the combined map
combined_map_crime
```


## 4.3 Income level (Community)

```{python}
# read the Chicago Community Shapefile
gdf = gpd.read_file('./VacantLots/CommaArea/CommAreas.shp')

# read Social Economic file
df_se = pd.read_csv('./data/socioeconomic.csv')

# Rename the column in df_se
df_se = df_se.rename(columns={'Community Area Number': 'AREA_NUMBE', 'PER CAPITA INCOME ': 'INCOME'})
```

```{python}
# Calculate centroids for each polygon in the geometry column
gdf['centroid'] = gdf.geometry.centroid

# Extract Longitude and Latitude from the centroid
gdf['Longitude'] = gdf['centroid'].x
gdf['Latitude'] = gdf['centroid'].y

# Drop the 'centroid' column as it contains Point objects
gdf = gdf.drop(columns=['centroid'])
```

```{python}
# Merge the shapefile GeoDataFrame with the crime data DataFrame
gdf_merged = gdf.merge(df_se, on='AREA_NUMBE', how='left')

# Convert the GeoDataFrame to GeoJSON format for Altair
gdf_merged_json = json.loads(gdf_merged.to_crs(epsg=4326).to_json())

# Create a base map layer
base_map = alt.Chart(alt.Data(values=gdf_merged_json['features'])).mark_geoshape(
    fill='white',
    stroke='gray',
    strokeWidth=0.5
).project(
    type='identity',
    reflectY=True 
).properties(
    width=400,
    height=400
)

# Create the choropleth layer for crime counts
choropleth = alt.Chart(alt.Data(values=gdf_merged_json['features'])).mark_geoshape(
    stroke='white',
    strokeWidth=0.5
).encode(
    color=alt.Color('properties.INCOME:Q', scale=alt.Scale(scheme='bluegreen'), title='Per Capita Income')
).project(
    type='identity',
    reflectY=True
)

# Combine the base map and choropleth layers
chro_map = (base_map + choropleth).configure_view(
    stroke=None  # Remove gridlines
)
```

```{python}
# Combine the choropleth map and scatter layers
combined_map_income = (chro_map + scatter).properties(
    title={
        'text': 'Per Capita Income by Community and Vacant Lands',
        'fontSize': 16,
        'anchor': 'middle'
    }
).configure_view(
    stroke=None  # Remove gridlines
)

# Display the combined map
combined_map_income
```


```{python}
# Combine the choropleth map and scatter layers
combined_map_income = (chro_map + scatter).properties(
    title={
        'text': 'Per Capita Income by Community and Vacant Lands',
        'fontSize': 16,
        'anchor': 'middle'
    }
).configure_view(
    stroke=None  # Remove gridlines
)

# Display the combined map
combined_map_income
```
